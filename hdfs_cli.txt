# pig library file needed
wget http://central.maven.org/maven2/com/googlecode/json-simple/json-simple/1.1.1/json-simple-1.1.1.jar
wget http://central.maven.org/maven2/com/twitter/elephantbird/elephant-bird-pig/4.14/elephant-bird-pig-4.14.jar
wget http://central.maven.org/maven2/com/twitter/elephantbird/elephant-bird-hadoop-compat/4.14/elephant-bird-hadoop-compat-4.14.jar
wget http://central.maven.org/maven2/org/apache/avro/avro/1.8.0/avro-1.8.0.jar
wget http://central.maven.org/maven2/org/apache/pig/piggybank/0.15.0/piggybank-0.15.0.jar
wget https://maven.repository.redhat.com/ga/org/codehaus/jackson/jackson-core-asl/1.9.13.redhat-3/jackson-core-asl-1.9.13.redhat-3.jar
wget https://maven.repository.redhat.com/ga/org/codehaus/jackson/jackson-mapper-asl/1.9.13.redhat-3/jackson-mapper-asl-1.9.13.redhat-3.jar

# create the flume destination directory on hdfs
hdfs dfs -mkdir -p /user/cloudera/output/hackerday-portalservice/flume/techJobTweet

# run the flume agent
flume-ng agent --name jobSearch --conf-file /home/cloudera/Classes/hadoop-training-projects/final_project_20_11_16/twitter-stream/live_demo.properties --classpath /home/cloudera/Classes/hadoop-training-projects/final_project_20_11_16/twitter-stream/flume-sources-1.0-SNAPSHOT.jar

# we used avro-tools to extract the schema from the avro file
avro-tools getschema part-m-00000.avro > twitter_jobs.avsc

# creating a hdfs directory for hold the avro schema
hdfs dfs -mkdir -p /user/cloudera/output/hackerday-portalservice/pig/twitterjobs_avro_schema

# loading the avro schema to hdfs
hdfs dfs -put twitter_jobs.avsc /user/cloudera/output/hackerday-portalservice/pig/twitterjobs_avro_schema